{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad25e089",
   "metadata": {},
   "source": [
    "## Métricas de Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a8f0c",
   "metadata": {},
   "source": [
    "### Métricas de Regresión "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a929e5",
   "metadata": {},
   "source": [
    "#### MSE: Error cuadrático medio - Mean Square Error\n",
    "\n",
    "En estadística es un estimador que mide el cálculo del promedio de la diferencia de los errores cuadrados predichos o estimados y esperados para las etiquetas de un conjunto de datos. Matemáticamente se calcula de la siguiente forma:\n",
    "\n",
    "$MSE = \\frac{1}{n}\\sum(\\overset{\\text{^}}{x}_i-x_i)^2$\n",
    "\n",
    "Donde $n$ es el número de datos, $x_i$ representa los valores observados y $\\overset{\\text{^}}{x}$ los valores predichos.\n",
    "\n",
    "<img src=\"MSE-Graph.PNG\" width=\"350\"/>\n",
    "<img src=\"MSE-Graph2.PNG\" width=\"350\"/>\n",
    "\n",
    "El MSE es una función que mide la probabilidad de que a una variable le ocurra cierta transformación ante ciertas condiciones esto corresponde al valor esperado de la perdida cuadrática, la diferencia se debe a la aleatoriedad de la muestra o porque el estimador carece de datos que le permita producir una estimación con mayor presición. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4438236f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.41666667, 1.        ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cálculo de MSE con Sciki learn:\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_estimada = [3, -0.5, 2, 7]\n",
    "y_predicha = [2.5, 0.0, 2, 8]\n",
    "mean_squared_error(y_true, y_pred)\n",
    "\n",
    "y_estimada = [3, -0.5, 2, 7]\n",
    "y_predicha = [2.5, 0.0, 2, 8]\n",
    "mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "y_estimada = [[0.5, 1],[-1, 1],[7, -6]]\n",
    "y_predicha = [[0, 2],[-1, 2],[8, -5]]\n",
    "mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ed2f201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8227486121839513"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_estimada, y_predicha, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cde8974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.41666667, 1.        ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_estimada, y_predicha, multioutput='raw_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "977e3bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.825"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_estimada, y_predicha, multioutput=[0.3, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77229516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_estimada:\n",
      " tensor([[ 0.5000,  1.0000],\n",
      "        [-1.0000,  1.0000],\n",
      "        [ 7.0000, -6.0000]])\n",
      "y_predicha:\n",
      " tensor([[ 0,  2],\n",
      "        [-1,  2],\n",
      "        [ 8, -5]])\n",
      "MSE loss: tensor(0.7083)\n"
     ]
    }
   ],
   "source": [
    "#Cálculo con Pytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# define the input and target tensors\n",
    "y_estimada = torch.tensor([[0.5, 1],[-1, 1],[7, -6]])\n",
    "y_predicha = torch.tensor([[0, 2],[-1, 2],[8, -5]])\n",
    "\n",
    "# print input and target tensors\n",
    "print(\"y_estimada:\\n\", y_estimada)\n",
    "print(\"y_predicha:\\n\", y_predicha)\n",
    "\n",
    "# create a criterion to measure the mean squared error\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# compute the loss (mean squared error)\n",
    "output = mse(y_estimada, y_predicha)\n",
    "\n",
    "# output.backward()\n",
    "print(\"MSE loss:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f3f42",
   "metadata": {},
   "source": [
    "https://www.tutorialspoint.com/how-to-measure-the-mean-squared-error-squared-l2-norm-in-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaffc54d",
   "metadata": {},
   "source": [
    "####  RMSE: Raíz de la Media Cuadrática - Root Mean Squared Error\n",
    "\n",
    "Es normalmente usada para medir la diferencia entre los valores predichos por un modelo y los valores observados. Cuando el RMSE es calculado las unidades son iguales al valor target predicho, o tienen una misma escala. Matemáticamente se representa de la siguiente manera:\n",
    "\n",
    "$RMSD = \\sqrt\\frac{\\sum^N_{i=1}(x_i-\\overset{\\text{^}}{x})^2}{N}$\n",
    "\n",
    "Donde $x_i$ es el valor esperado del conjunto de datos, $\\overset{\\text{^}}{x}$ es el valor predicho, expresado en unidades y no unidades cuadradadas a diferencia del MSE, entre $N$ que es el total de las muestras.\n",
    "\n",
    "$RMSE = \\sqrt{MSE} $\n",
    "\n",
    "El RSMD sirve para mostrar un agregado de las magnitudes de los errores en las predicciones de los elementos identificables en un conjunto de datos a una medida de capacidad predictiva, por lo cual es una medida de exactitud para comparar errores de pronósticos  de diferentes modelos para un conjunto de datos particular y no entre conjuntos de datos. \n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "<img src=\"RMSE.PNG\" width=\"450\"/>\n",
    "\n",
    "Las barras de error indican un intervalo de confianza del 95 % calculado con el error cuadrático medio del término de interacción de cada análisis de varianza. (REFERENCIA)\n",
    "\n",
    "Un resultado perfecto de RMSE es 0.0 que significa que todas las predicciones son iguales a las esperadas o igual a un intervalo de confianza de 100%, lo cual no es usual que suceda e implica trivialidad en el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3394ad98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7083333333333334"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cálculo de RMSE con Sciki learn:\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math \n",
    "\n",
    "y_estimada = [3, -0.5, 2, 7]\n",
    "y_predicha = [2.5, 0.0, 2, 8]\n",
    "mean_squared_error(y_true, y_pred)\n",
    "\n",
    "y_estimada = [3, -0.5, 2, 7]\n",
    "y_predicha = [2.5, 0.0, 2, 8]\n",
    "mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "y_estimada = [[0.5, 1],[-1, 1],[7, -6]]\n",
    "y_predicha = [[0, 2],[-1, 2],[8, -5]]\n",
    "mean_squared_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5a2ed6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9070549113388623\n"
     ]
    }
   ],
   "source": [
    "MSE = mean_squared_error(y_estimada, y_predicha, squared=False)\n",
    "RMSE = math.sqrt(MSE)\n",
    "\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb41cd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9082951062292475\n"
     ]
    }
   ],
   "source": [
    "rv_MSE = mean_squared_error(y_estimada, y_predicha, multioutput=[0.3, 0.7])\n",
    "rv_RMSE = math.sqrt(rv_MSE)\n",
    "\n",
    "print(rv_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ec2371b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_estimada:\n",
      " tensor([[ 0.5000,  1.0000],\n",
      "        [-1.0000,  1.0000],\n",
      "        [ 7.0000, -6.0000]])\n",
      "y_predicha:\n",
      " tensor([[ 0,  2],\n",
      "        [-1,  2],\n",
      "        [ 8, -5]])\n",
      "RMSE loss: 0.8416253997266946\n"
     ]
    }
   ],
   "source": [
    "#Cálculo con Pytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# define the input and target tensors\n",
    "y_estimada = torch.tensor([[0.5, 1],[-1, 1],[7, -6]])\n",
    "y_predicha = torch.tensor([[0, 2],[-1, 2],[8, -5]])\n",
    "\n",
    "# print input and target tensors\n",
    "print(\"y_estimada:\\n\", y_estimada)\n",
    "print(\"y_predicha:\\n\", y_predicha)\n",
    "\n",
    "# create a criterion to measure the mean squared error\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# compute the loss (mean squared error)\n",
    "output = mse(y_estimada, y_predicha)\n",
    "\n",
    "RMSE = math.sqrt(output)\n",
    "\n",
    "# output.backward()\n",
    "print(\"RMSE loss:\", RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf9c793",
   "metadata": {},
   "source": [
    "#### MAE: Error Medio Absoluto - Mean Absolute Error\n",
    "\n",
    "El error medio absoluto es una técnica de medida que calcula la suma absoluta de errores dividos entre el tamaño total de la muestra. ¿Qué se considera \"error\"? Estadísticamente hablando, el error se refiere a la desviación de un valor observado con respecto a la media de una muestra. \n",
    "\n",
    "Por lo tanto MAE es una medida de errores entre observaciones aparejadas que describen un mismo fenómeno. En un sentido de una comparación entre los valores observados y predichos.\n",
    "\n",
    "Se describe con la siguiente fórmula: \n",
    "\n",
    "$MAE = \\frac{\\sum^n_{1=i}{|y_i - x_i|}}{n} = \\frac{\\sum^n_{1=i}{|e_i|}}{n}$\n",
    "\n",
    "Donde $y$ es la predicción y $x$ el valor verdadero, $n$ el total de la muestra. \n",
    "\n",
    "<img src=\"MAE1.PNG\" width=\"450\"/>\n",
    "\n",
    "El anterior es una caracterización del pronóstico de errores y evaluación compativa de ejemplo de un pronóstico del uso de energía renobable, donde se pueden ver las varianzas de los errores, esto al ser divido por el total de la muestra nos permite comparar el dato, dado que se busca un solo resultado único con los valores reales.\n",
    "\n",
    "Junto con el MSE y el RMSE son las métricas más populares para evaluar un modelo de regresión, pero a diferencia de las anteariores que castigan errores grandes, la MAE crece de manera líneal. Entre más cerca al 1 más exacto es el pronóstico en comparación al valor real, pero al exactitud total debería analizarse el tamaño de los datos o el diseño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a671e951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "y_real = [3, -0.5, 2, 7]\n",
    "y_pronosticado = [2.5, 0.0, 2, 8]\n",
    "median_absolute_error(y_real, y_pronosticado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f5758ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_real = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pronosticado = [[0, 2], [-1, 2], [8, -5]]\n",
    "median_absolute_error(y_real, y_pronosticado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "799f5119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 1. ])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_absolute_error(y_real, y_pronosticado, multioutput='raw_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cceae963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_absolute_error(y_real, y_pronosticado, multioutput=[0.3, 0.7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ed947b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor:\n",
      " tensor([[-0.3015,  0.4134, -0.9815, -0.2512],\n",
      "        [ 0.4303, -0.1585,  0.2871,  0.4258],\n",
      "        [ 0.4995, -0.2316,  0.4205,  1.4265]])\n",
      "Target Tensor:\n",
      " tensor([[-1.8159,  0.9825, -1.3881, -0.1789],\n",
      "        [-0.9428,  1.6959,  0.2280,  1.1362],\n",
      "        [ 0.1317,  1.1636,  0.2543, -0.3708]])\n",
      "MAE loss: tensor(0.8572)\n"
     ]
    }
   ],
   "source": [
    "#L1LOSS: Implementación con Pytorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# define the input and target tensors\n",
    "input = torch.randn(3, 4)\n",
    "target = torch.randn(3, 4)\n",
    "\n",
    "print(\"Input Tensor:\\n\", input)\n",
    "print(\"Target Tensor:\\n\", target)\n",
    "\n",
    "mae = nn.L1Loss()\n",
    "\n",
    "output = mae(input, target)\n",
    "\n",
    "print(\"MAE loss:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6ff785",
   "metadata": {},
   "source": [
    "#### MAPE: Error Porcentual Absoluto Medio \n",
    "\n",
    "Dada que los problemas de regresión buscan encontrar un modelo donde se pueda compara los valores verdaderos y un pronóstico, que se puede representar como: $Z=(X,Y)$, donde se pueda medir que tan cerca está Y de X. Su representación matemática se ve así:\n",
    "\n",
    "$MAPE = \\frac{100\\%}{n}\\sum^n_{t=1}|\\frac{A_t - F_t}{A_t}|$\n",
    "\n",
    "Donde $A_t$ es el valor observado verdadero y $F_t$ es el valor pronosticado, su diferencia es dividida por $A_t$, el valor absoluto se presenta como un radio, que es sumado para cada uno de los valores pronosticados y divido por $n$.\n",
    "\n",
    "En el siguiente ejemplo podemos ver el MAPE de varias acciones para varios modelos de regresión: \n",
    "\n",
    "<img src=\"MAPE.PNG\" width=\"450\"/>\n",
    "\n",
    "Donde cada línea expresa los diversos errores porcentuales para cada acción espeficada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b7757e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3273809523809524"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "y_real = [3, -0.5, 2, 7]\n",
    "y_pronosticado = [2.5, 0.0, 2, 8]\n",
    "mean_absolute_percentage_error(y_real, y_pronosticado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f3990731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5515873015873016"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_real = [[0.5, 1], [-1, 1], [7, -6]]\n",
    "y_pronosticado = [[0, 2], [-1, 2], [8, -5]]\n",
    "mean_absolute_percentage_error(y_real, y_pronosticado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e62b71e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6198412698412699"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_percentage_error(y_real, y_pronosticado, multioutput=[0.3, 0.7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "37f62a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112589990684262.48"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cuando y_real es cero el valor puede ser elevadamente alto por lo cual se divide por epsilon\n",
    "\n",
    "y_true = [1., 0., 2.4, 7.]\n",
    "y_pred = [1.2, 0.1, 2.4, 8.]\n",
    "mean_absolute_percentage_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106feed",
   "metadata": {},
   "source": [
    "#### (R²): R cuadrado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f217fde",
   "metadata": {},
   "source": [
    "### Métricas de Regresión "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6331b34d",
   "metadata": {},
   "source": [
    "#### Matriz de confusión\n",
    "\n",
    "La matriz de confusión es una tabla que describe el rendimiento de un modelo de clasificación en un conjunto de datos de prueba cuyos valores verdaderos se conocen. Esta matriz tiene una dimensión de 2x2 con los valores reales en un eje y los predichos en otro.\n",
    "\n",
    "<img src=\"https://www.juanbarrios.com/wp-content/uploads/2019/07/MATRIZ-CONFUSION-400x358.png\" alt=\"Matriz de Confusión\" style=\"height: 300px; width:300px;\"/>\n",
    "\n",
    "Como se puede observar en la imagen anterior, la matriz de confusión presenta 4 opciones que son los siguientes:\n",
    "- Verdadero positivo (TP):  el modelo predice correctamente la clase positiva (la predicción y la realidad son ambas positivas). \n",
    "- Verdadero negativo (TN): el modelo predice correctamente la clase negativa (tanto la predicción como la realidad son negativas). \n",
    "- Falso negativo (FP):el modelo da una predicción errónea de la clase negativa (predicción-positiva, real-negativa). \n",
    "- Falso positivo (FN):el modelo predice erróneamente la clase positiva (predicho-negativo, real-positivo)\n",
    "\n",
    "Los siguientes son ejemplos de las opciones:\n",
    "- Verdadero positivo (TP):Una persna tiene cáncer y la prueba la demustra\n",
    "- Verdadero negativo (TN): Una persona no tiene cáncer pero la prueba demuestra que no\n",
    "- Falso negativo (FP): Un persona tiene cáncer pero la prueba demuestra que no\n",
    "- Falso positivo (FN): La persona no tiene cáncer pero la prueba demuestra que si\n",
    "\n",
    "<img src=\"https://images3.programmerclick.com/705/47/47733fd252edf6b399762ebe5eb71d31.png\" alt=\"Matriz de Confusión\" style=\"height: 300px; width:300px;\"/>\n",
    "\n",
    "A partir de las opciones anteriores surgen métricas de la matriz de confusión:\n",
    "- Precisión (Precision) y exactitud (Accuracy)\n",
    "- Sensibilidad (Recall) y especificidad (Especificity)\n",
    "\n",
    "Precisión y exactitud \n",
    "\n",
    "Según la página Health Big Data, la precisión se refiere a la dispersión del conjunto de valores obtenidos a partir de mediciones repetidas de una magnitud. Cuanto menor es la dispersión mayor la precisión. \n",
    "Se representa por la proporción de verdaderos positivos dividido entre todos los resultados positivos, es decir la suma de los verdaderos positivos y falsos positivos. La siguiente imagen es la fórmula:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/414/1*tHf0NHWGMwZTNe7TkylyYA.png\" alt=\"Matriz de Confusión\" style=\"height: 150px; width:150px;\"/>\n",
    "\n",
    "Por otro lado, la exactitud está relacionada con el sesgo de una estimación y se refiere que tan cerca está el resultado de una medición del valor verdadero. Se representa como la proporción de resultados verdaderos (tanto verdaderos positivos (VP) como verdaderos negativos (VN)) dividido entre el número total de casos examinados. La siguiente imagen es la fórmula:\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/48051257888_760554732e_b.jpg\" alt=\"Matriz de Confusión\" style=\"height: 300px; width:450px;\"/>\n",
    "\n",
    "###### Sensibilidad (Recall) Especificidad (Especificity)\n",
    "\n",
    "La sensibilidad nos indica el porcentaje que son positivos predicjos del total de positivos, es como verdaderos positivos como se puede observar en la siguiente imagen: \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/444/1*ynGz0zu8ZlpgZ5voYRKfLg.png\" alt=\"Matriz de Confusión\" style=\"height: 150px; width:150px;\"/>\n",
    "\n",
    "En el caso de la espeficidad se refiere los casos negativos que fueron calificados correctamente.\n",
    "\n",
    "<img src=\"https://geekymedics.com/wp-content/uploads/2018/06/Specificity-equation.jpg\" alt=\"Matriz de Confusión\" style=\"height: 200px; width:200px;\"/>\n",
    "\n",
    "###### Ejemplo de matriz de confusión\n",
    "\n",
    "El siguiente es un ejemplo simple que fue implementado con Sklearn, en este caso se usó dos arreglos, uno con datos actuales/reales y otro con datos predecidos alambrados.Al llamar la función confusion_matrix con los dos arreglos obtenemos la respuesta en formato de matriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d84a3738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 1],\n",
       "       [3, 4]], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "Valoes_reales = ['F','T','T','F','T','F','T','T','F','T','F','T']\n",
    "Valores_pred = ['F','F','T','F','T','F','F','T','F','T','T','F']\n",
    "confusion_matrix(Valoes_reales,Valores_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e3ee27",
   "metadata": {},
   "source": [
    "Al realizar los cálculos manualmente se puede decir que la matriz mostrada anteriormente fue calculada correctamente, y consideramos que es una forma muy simple de comprobar como está el modelo para predecir los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0335f43c",
   "metadata": {},
   "source": [
    "#### F1 Score\n",
    "\n",
    "Según diferentes paǵinas del internet, F1 es una propuesta de mejora de dos métricas de rendimiento más sencillas.  El F1 combina las medidas más comunes de precision y recall en un sólo valor, esto hace que el proceso sea más simple de poder realizar comparaciones combinando las medidas. Esta métrica ha sido diseñad para trabajar con datos desequilibrados.\n",
    "\n",
    "###### F1 Score Formula\n",
    "\n",
    "La siguiente es la fórmula de F1 realizando combinación entre la precisión y recall. \n",
    "\n",
    "<img src=\"https://lawtomated.com/wp-content/uploads/2019/10/F1-Score.png\" alt=\"Matriz de Confusión\" />\n",
    "\n",
    "Se debe considedrar los siguientes puntos al utilizar F1 Score para evaluar un modelo:\n",
    "- Sí la precisión y recall son altas entonces el modelo obtendrá una puntuación F1 alta \n",
    "- Sí la precisión y recall son bajas entonces el modelo obtendrá una puntuación F1 baja \n",
    "- Sí la precisión es baja y recall alta entonces el modelo obtendrá una puntuación F1 media \n",
    "\n",
    "###### Precisión\n",
    "\n",
    "Precisión puede ser una métrica individual para evaluar modelo pero es utilizada y parte de F1 score. La siguiente es la fórmula de precisón:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/622/1*hIPkZ3Su21sUdwi_A-arUg.png\" alt=\"Matriz de Confusión\" />\n",
    "\n",
    "Según Joos Korstanje, la fórmula de precisión se puede interpretar que todo lo que se ha predicho como positivo, la precisión cuenta el porcentaje que es correcto:\n",
    "- Un modelo no preciso puede encontrar muchos de los positivos, pero su método de selección es ruidoso: también detecta erróneamente muchos positivos que no son realmente positivos.\n",
    "- Un modelo preciso es muy \"puro\": puede que no encuentre todos los positivos, pero los que el modelo clasifica como positivos tienen muchas probabilidades de ser correctos\n",
    "\n",
    "###### Recall\n",
    "\n",
    "Igual que precisón, la sensibilidad que se conoce como recall puede ser una métrica individual pero es un componente fundamental para el cálculo de F1. La siguiente es la fórmula de recall:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/612/1*8kzRR5Pl1SI2tRUa3CvNsA.png\" alt=\"Matriz de Confusión\" />\n",
    "\n",
    "Un modelo que tiene un resultado de recall alto logra encontrar todos los casos positivos de los datos, pero también se puede suceder que algunos casos negativos que los identifican como positivos. Del mismo modo, si el modelo que tiene un resultado de recall bajo no es capaz de enfcontrar la mayoría de los casos positivos en los datos. \n",
    "\n",
    "Un ejemplo en la vida real, normalmente cuando compramos frutas hay ocasiones que hay partes que están podridas, y al vendor sólo le interesa asegurar de que los clientes devuelven las frutas podridas y no le importa si los clientes devuelven frutas no podridas, es decir que la precisión no es tan importante en este caso. \n",
    "\n",
    "###### Ejemplo de F1 Score\n",
    "\n",
    "El siguiente es un ejemplo simple que fue implementado con Sklearn, en este caso se usó dos arreglos, uno con datos actuales/reales y otro con datos predecidos alambrados.Al llamar la función f1_score con los dos arreglos obtenemos 0.6666 como respuesta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "356be0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "Valoes_reales = ['F','T','T','F','T','F','T','T','F','T','F','T']\n",
    "Valores_pred = ['F','F','T','F','T','F','F','T','F','T','T','F']\n",
    "f1_score(Valoes_reales,Valores_pred,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5261d6",
   "metadata": {},
   "source": [
    "Como se ha mencionado anteriormente, si el resultado de F1 score es alto es mejor, pero igual en este caso estamos usando datos alambrados de una sola dimensión con una cantidad pequeña de datos, pero el resultado obtenido debe depender del problema de predicción para poder decir con certeza de sí es bueno o malo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04590907",
   "metadata": {},
   "source": [
    "#### Accuracy (Exactitud)\n",
    "\n",
    "La accuracy que se conoce como exactitud es otra métrica de clasificación más conocida ya que esta puede ser utilizada individualmente o ser parte de otras métricas. Esta métrica es muy simple y puede ser utilizada tanta para los problemas de clasificación binarios como a los multiclase. Se recomienda utilizar la métrica exactitud para los problemas de clasificación que están equilibrados y no sesgados o no hay desequilibrio de clases.Como se puede ver en la siguiente fórmula, la exactitud es la proporción de resultados verdaderos entre el número total de casos.\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/48051257888_760554732e_b.jpg\" alt=\"Matriz de Confusión\"/>\n",
    "\n",
    "###### Ejemplo de Recall\n",
    "\n",
    "El siguiente es un ejemplo simple que fue implementado con Sklearn, en este caso se usó dos arreglos, uno con datos actuales/reales y otro con datos predecidos alambrados.Al llamar la función accuracy_score con los dos arreglos obtenemos 0.6666 como respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7355e2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "Valoes_reales = ['F','T','T','F','T','F','T','T','F','T','F','T']\n",
    "Valores_pred = ['F','F','T','F','T','F','F','T','F','T','T','F']\n",
    "accuracy_score(Valoes_reales,Valores_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b49ceb0",
   "metadata": {},
   "source": [
    "Como ya se ha mencionada anteriormente, la accuracy es una métrica representa el número de casos de datos correctamente clasificados sobre el número total de casos de datos.En este caso obtuvimos que casi 67% de las predicciones fueron correctas. Esta métrica mientras más alto sea el valor es mejor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303db1a8",
   "metadata": {},
   "source": [
    "#### Recall (Exhaustividad)\n",
    "\n",
    "El recall es una métrica que trata de capturar el mayor número posible de positivos. Un ejemplo que puede utilizar esta métrica es construir un modelo que puede predecir si la persona tiene Covid o no, ya que se requiere capturar aunque no estemos seguros ya que uno puede ser asintómatico. Es decir que Recall da una medida con la que el modelo es capaz de identificar los datos relevantes.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/612/1*8kzRR5Pl1SI2tRUa3CvNsA.png\" alt=\"Matriz de Confusión\" />\n",
    "\n",
    "###### Ejemplo de recall\n",
    "\n",
    "El siguiente es un ejemplo simple que fue implementado con Sklearn, en este caso se usó dos arreglos, uno con datos actuales/reales y otro con datos predecidos alambrados.Al llamar la función recall_score con los dos arreglos obtenemos 0.6666 como respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "934d217a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6857142857142857"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "Valoes_reales = ['F','T','T','F','T','F','T','T','F','T','F','T']\n",
    "Valores_pred = ['F','F','T','F','T','F','F','T','F','T','T','F']\n",
    "recall_score(Valoes_reales,Valores_pred,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d2fe8d",
   "metadata": {},
   "source": [
    "#### Precision (Precisión)\n",
    "\n",
    "Según el internet la métrica precisión sirve para realizar clasificación de los datos desquilibrados.Como ya se ha mencionado anteriormente, precisión puede ser una métrica individual y también ser componente de otras métricas como F1 Score y Matriz de confusión. La precisión es una métrica que cuantifica el número de predicciones positivas correctas obtenidas. Se calcula como la proporción de ejemplos positivos predichos correctamente dividida por el número total de ejemplos positivos que se predijeron.\n",
    "\n",
    "Se recomienda utilizar la métrica precisión cuando se requiere estar muy seguro de la predicción. Un ejemplo real, es que cuando hay un empleado nuevo en la empresa se debe asignarle los accesos correctos para no romper ninguna política de seguridad. Este métrica puede ser utilizada tanto para la clasificación binaria como clasificación multiclases.\n",
    "\n",
    "###### Precisión para la clasificación binaria\n",
    "\n",
    "La precisión se calcula como el número de verdaderos positivos dividido por el número total de verdaderos positivos y falsos positivos. El rango del resultado es un valor entre 0 para una precisión nula y 1 para una precisión total o perfecta, es decir mientras más alto sea el valor el resultado será mejor. La siguiente es la fórmula de precisón:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/622/1*hIPkZ3Su21sUdwi_A-arUg.png\" alt=\"Matriz de Confusión\" />\n",
    "\n",
    "###### Precisión para la clasificación multiclases\n",
    "\n",
    "La precisión es una métrica muy flexible ya que nos permite evaluar modelo que son de clasificación multiclase, y esta  se calcula como la suma de verdaderos positivos en todas las clases dividida por la suma de verdaderos positivos y falsos positivos en todas las clases.\n",
    "\n",
    "###### Ejemplo de precisión\n",
    "\n",
    "El siguiente es un ejemplo simple que fue implementado con Sklearn, en este caso se usó dos arreglos, uno con datos actuales/reales y otro con datos predecidos alambrados.Al llamar la función precision_score con los dos arreglos obtenemos 0.6666 como respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e231523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6857142857142857"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "Valoes_reales = ['F','T','T','F','T','F','T','T','F','T','F','T']\n",
    "Valores_pred = ['F','F','T','F','T','F','F','T','F','T','T','F']\n",
    "precision_score(Valoes_reales,Valores_pred,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3f5abc",
   "metadata": {},
   "source": [
    "Como ya se ha mencionada anteriormente, la precisión es una métrica de evalua las predicciones positivas realizadas que son correctas.En este caso obtuvimos que casi 69% de las predicciones fueron correctas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894ae667",
   "metadata": {},
   "source": [
    "Referencias.\n",
    "\n",
    "AWS. (s.f.). AWS > Documentation > Amazon Machine Learning. Obtenido de https://docs.aws.amazon.com/es_es/machine-learning/latest/dg/evaluating-model-accuracy.html\n",
    "\n",
    "Brownlee, J. (s.f.). Machine Learning Mastery. Obtenido de Regression Metrics for Machine Learning.\n",
    "\n",
    "Research Gate. (s.f.). Figure 5 - uploaded by Alinda Friedman. Obtenido de https://www.researchgate.net/figure/Root-mean-square-error-RMSE-from-the-center-of-the-circle-for-the-one-glyph-task-in_fig3_309026476\n",
    "\n",
    "Research Gate. (s.f.). Research Gate. Obtenido de https://www.researchgate.net/figure/Comparison-graph-of-model-performance-in-the-Mean-Absolute-Percentage-Error-MAPE_fig2_343349809\n",
    "\n",
    "Sanjuán, F. J. (s.f.). Economipedia. Obtenido de R cuadrado ajustado (Coeficiente de determinación ajustado): https://economipedia.com/definiciones/r-cuadrado-ajustado-coeficiente-de-determinacion-ajustado.html#:~:text=El%20R%20cuadrado%20ajustado%20(o,en%20explicar%20la%20variable%20dependiente.\n",
    "\n",
    "Science Direct. (s.f.). Obtenido de Mean Absolute Error: https://www.sciencedirect.com/topics/engineering/mean-absolute-error\n",
    "\n",
    "Scikit Learn. (s.f.). API Reference. Obtenido de https://scikit-learn.org/stable/modules/classes.html#regression-metrics\n",
    "\n",
    "Sitio Big Data. (s.f.). Aprendizaje automático de métricas de regresión (MSE). Obtenido de https://sitiobigdata.com/2019/05/27/modelos-de-machine-learning-metricas-de-regresion-mse-parte-2/#\n",
    "\n",
    "Statistics How To. (s.f.). Absolute Error & Mean Absolute Error (MAE). Obtenido de https://www.statisticshowto.com/absolute-error/\n",
    "\n",
    "Statistics How To. (s.f.). Mean Absolute Percentage Error (MAPE). Obtenido de https://www.statisticshowto.com/mean-absolute-percentage-error-mape/\n",
    "\n",
    "Statologos. (s.f.). Error porcentual absoluto medio (MAPE). Obtenido de https://statologos.com/mapa-de-error-de-porcentaje-absoluto-medio/\n",
    "\n",
    "Tutorials Point. (s.f.). Tutorials Point. Obtenido de How to measure the mean squared error(squared L2 norm) in PyTorch?: https://www.tutorialspoint.com/how-to-measure-the-mean-squared-error-squared-l2-norm-in-pytorch\n",
    "\n",
    "Health Big Data. (2019, 26 de julio). La matriz de confusión y sus métricas. https://www.juanbarrios.com/la-matriz-de-confusion-y-sus-metricas/\n",
    "\n",
    "Python Guides. (2022, 1 de febrero). Scikit learn Confusion Matrix. https://pythonguides.com/scikit-learn-confusion-matrix/\n",
    "\n",
    "Joos Korstanje. (2021, 31 de agosto). The F1 score. https://towardsdatascience.com/the-f1-score-bec2bbc38aa6\n",
    "\n",
    "scikit learn. (s.f.). F1_Score. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "\n",
    "AskPython. (s.f.). Precision and Recall in Python. https://www.askpython.com/python/examples/precision-and-recall-in-pythonc\n",
    "\n",
    "Jason Brownlee. (2020, 3 de enero). How to Calculate Precision, Recall, and F-Measure for Imbalanced Classification. https://machinelearningmastery.com/precision-recall-and-f-measure-for-imbalanced-classification/\n",
    "\n",
    "Rahul Agarwal. (2019, 17 de septiembre). The 5 Classification Evaluation metrics every Data Scientist must know.https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
